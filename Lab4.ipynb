{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs and launch test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "__global__ void multiply_them(float *dest, float *a, float *b)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  dest[i] = a[i] * b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "a = np.random.randn(400).astype(np.float32)\n",
    "b = np.random.randn(400).astype(np.float32)\n",
    "\n",
    "# Allocate memory on the device\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "dest_gpu = cuda.mem_alloc(a.nbytes)\n",
    "\n",
    "# Copy data from host to device\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "# Execute the kernel\n",
    "func = mod.get_function(\"multiply_them\")\n",
    "func(dest_gpu, a_gpu, b_gpu, block=(400,1,1))\n",
    "\n",
    "# Copy result from device to host\n",
    "dest = np.empty_like(a)\n",
    "cuda.memcpy_dtoh(dest, dest_gpu)\n",
    "\n",
    "# Display results\n",
    "print(dest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandelbrot set: CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "__device__ int mandelbrot(float real, float imag, int max_iter) {\n",
    "    float r = real;\n",
    "    float i = imag;\n",
    "    for (int t = 0; t < max_iter; t++) {\n",
    "        if (r * r + i * i > 4.0) return t;\n",
    "        float new_r = r * r - i * i + real;\n",
    "        i = 0;  // TODO: change 0 to correct code\n",
    "        r = new_r;\n",
    "    }\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void compute_mandelbrot(float *output, int width, int height, float x_min, float x_max, float y_min, float y_max, int max_iter) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    if (idx < width && idy < height) {\n",
    "        float real = x_min + idx * (x_max - x_min) / width;\n",
    "        float imag = 0;  // TODO: change 0 to correct code\n",
    "        output[idy * width + idx] = mandelbrot(real, imag, max_iter);\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "def cuda_mandelbrot(width, height, x_min, x_max, y_min, y_max, max_iter):\n",
    "    output = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    # Allocate memory on the device\n",
    "    output_gpu = cuda.mem_alloc(output.nbytes)\n",
    "\n",
    "    # Execute the kernel\n",
    "    block_size = (16, 16, 1)\n",
    "    grid_size = (width // block_size[0] + 1, height // block_size[1] + 1)\n",
    "\n",
    "    start = time.time()\n",
    "    func = mod.get_function(\"compute_mandelbrot\")\n",
    "    func(output_gpu, np.int32(width), np.int32(height), np.float32(x_min), np.float32(x_max), np.float32(y_min), np.float32(y_max), np.int32(max_iter), block=block_size, grid=grid_size)\n",
    "    cuda.Context.synchronize()  # Wait for the computation to finish\n",
    "    end = time.time()\n",
    "\n",
    "    # Copy the result back to the host\n",
    "    cuda.memcpy_dtoh(output, output_gpu)\n",
    "\n",
    "    return output, end - start\n",
    "\n",
    "width, height = 1024, 768\n",
    "x_min, x_max = -2.0, 1.0\n",
    "y_min, y_max = -1.5, 1.5\n",
    "max_iter = 128\n",
    "\n",
    "mandelbrot_image, time_cuda = cuda_mandelbrot(width, height, x_min, x_max, y_min, y_max, max_iter)\n",
    "print(\"CUDA Time:\", time_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandelbrot set: Numpy (cpu) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_mandelbrot(width, height, x_min, x_max, y_min, y_max, max_iter):\n",
    "    output = np.zeros((height, width), dtype=np.float32)\n",
    "    for y in range(height):\n",
    "        imag = y_min + y * (y_max - y_min) / height\n",
    "        for x in range(width):\n",
    "            real = 0  # TODO: change 0 to correct code\n",
    "            r, i = real, imag\n",
    "            for t in range(max_iter):\n",
    "                if r * r + i * i > 4.0:\n",
    "                    output[y, x] = t\n",
    "                    break\n",
    "                r = 0  # TODO: change 0 to correct code\n",
    "                i = 2 * r * i + imag\n",
    "\n",
    "    return output\n",
    "\n",
    "start = time.time()\n",
    "mandelbrot_image_numpy = numpy_mandelbrot(width, height, x_min, x_max, y_min, y_max, max_iter)\n",
    "end = time.time()\n",
    "time_numpy = end - start\n",
    "print(\"CPU Time:\", time_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(24, 8))\n",
    "axs[0].imshow(mandelbrot_image)\n",
    "axs[0].set_title(f\"CUDA. Time: {time_cuda}\")\n",
    "axs[1].imshow(mandelbrot_image_numpy)\n",
    "axs[1].set_title(f\"Numpy. Time: {time_numpy}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
